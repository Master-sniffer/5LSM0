{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-18T23:36:24.668250Z","iopub.status.busy":"2024-06-18T23:36:24.667832Z","iopub.status.idle":"2024-06-18T23:36:24.704634Z","shell.execute_reply":"2024-06-18T23:36:24.703363Z","shell.execute_reply.started":"2024-06-18T23:36:24.668219Z"},"trusted":true},"outputs":[],"source":["from IPython.display import clear_output\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T23:36:24.707042Z","iopub.status.busy":"2024-06-18T23:36:24.706716Z","iopub.status.idle":"2024-06-18T23:37:00.300335Z","shell.execute_reply":"2024-06-18T23:37:00.299054Z","shell.execute_reply.started":"2024-06-18T23:36:24.707014Z"},"trusted":true},"outputs":[],"source":["!pip install wandb -qU\n","\n","import wandb"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T23:37:00.302650Z","iopub.status.busy":"2024-06-18T23:37:00.302192Z","iopub.status.idle":"2024-06-18T23:37:03.325822Z","shell.execute_reply":"2024-06-18T23:37:03.324599Z","shell.execute_reply.started":"2024-06-18T23:37:00.302599Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import gc\n","\n","from torch.nn import Sequential\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T23:37:03.327782Z","iopub.status.busy":"2024-06-18T23:37:03.327256Z","iopub.status.idle":"2024-06-18T23:37:16.107334Z","shell.execute_reply":"2024-06-18T23:37:16.106030Z","shell.execute_reply.started":"2024-06-18T23:37:03.327750Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install torchsummary -q\n","from torchsummary import summary"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T23:37:16.128921Z","iopub.status.busy":"2024-06-18T23:37:16.128550Z","iopub.status.idle":"2024-06-18T23:37:16.177032Z","shell.execute_reply":"2024-06-18T23:37:16.175929Z","shell.execute_reply.started":"2024-06-18T23:37:16.128890Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c, dropout_rate=0.4):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_c)\n","        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_c)\n","        self.relu = nn.LeakyReLU()\n","        self.residual = nn.Conv2d(in_c, out_c, kernel_size=1, padding=0)\n","        self.dropout = nn.Dropout2d(dropout_rate)\n","\n","    def forward(self, inputs):\n","        residual = self.residual(inputs)\n","        x = self.conv1(inputs)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x += residual\n","        x = self.dropout(x)\n","        x = self.relu(x)\n","        return x\n","\n","class dense_block(nn.Module):\n","    def __init__(self, in_channels, out_channels, dropout_rate=0.4):\n","        super(dense_block, self).__init__()\n","        self.linear = nn.Linear(in_channels, out_channels)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.size()\n","        x = x.view(batch_size, channels, height * width)\n","        x = x.transpose(1, 2)\n","        x = self.linear(x)\n","        x = self.dropout(x)\n","        x = x.transpose(1, 2)\n","        x = x.view(batch_size, channels, height, width)\n","        return x\n","\n","class SEBlock(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super(SEBlock, self).__init__()\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        y = self.global_avg_pool(x)\n","        y = self.fc1(y)\n","        y = self.relu(y)\n","        y = self.fc2(y)\n","        y = self.sigmoid(y)\n","        return x * y\n","\n","class SpatialAttention(nn.Module):\n","    def __init__(self):\n","        super(SpatialAttention, self).__init__()\n","        self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x = torch.cat([avg_out, max_out], dim=1)\n","        x = self.conv1(x)\n","        return self.sigmoid(x)\n","\n","class ChannelAttention(nn.Module):\n","    def __init__(self, in_planes, ratio=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, kernel_size=1, padding=0, bias=False)\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, kernel_size=1, padding=0, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n","        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n","        out = avg_out + max_out\n","        return self.sigmoid(out)\n","\n","class AttentionGate(nn.Module):\n","    def __init__(self, F_g, F_l, F_int):\n","        super(AttentionGate, self).__init__()\n","        self.W_g = nn.Sequential(\n","            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(F_int)\n","        )\n","\n","        self.W_x = nn.Sequential(\n","            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(F_int)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, g, x):\n","        g1 = self.W_g(g)\n","        x1 = self.W_x(x)\n","        psi = self.relu(g1 + x1)\n","        psi = self.psi(psi)\n","        return x * psi\n","\n","class PyramidPoolingModule(nn.Module):\n","    def __init__(self, in_channels, pool_sizes):\n","        super(PyramidPoolingModule, self).__init__()\n","        self.stages = nn.ModuleList([nn.AdaptiveAvgPool2d(pool_size) for pool_size in pool_sizes])\n","        self.convs = nn.ModuleList([nn.Conv2d(in_channels, in_channels // len(pool_sizes), kernel_size=1) for _ in pool_sizes])\n","        self.conv1x1 = nn.Conv2d(in_channels * 2, in_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        h, w = x.size(2), x.size(3)\n","        pyramids = [F.interpolate(self.convs[i](stage(x)), size=(h, w), mode='bilinear', align_corners=True) for i, stage in enumerate(self.stages)]\n","        out = torch.cat([x] + pyramids, dim=1)\n","        out = self.conv1x1(out)\n","        return out\n","\n","class FeatureFusionBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(FeatureFusionBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x1, x2):\n","        x = torch.cat([x1, x2], dim=1)\n","        x = self.conv(x)\n","        x = self.relu(x)\n","        return x\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c, dropout_rate=0.4):\n","        super().__init__()\n","        self.conv = conv_block(in_c, out_c, dropout_rate)\n","        self.pool = nn.MaxPool2d((2, 2))\n","        self.se = SEBlock(out_c)\n","        self.channel_attention = ChannelAttention(out_c)\n","        self.spatial_attention = SpatialAttention()\n","\n","    def forward(self, inputs):\n","        x = self.conv(inputs)\n","        x = self.se(x)\n","        x = self.channel_attention(x) * x\n","        x = self.spatial_attention(x) * x\n","        p = self.pool(x)\n","        return x, p\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c, dropout_rate=0.4):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv = conv_block(out_c + out_c, out_c, dropout_rate)\n","        self.attention = AttentionGate(F_g=out_c, F_l=out_c, F_int=out_c // 2)\n","        self.dense = dense_block(out_c, out_c, dropout_rate)\n","        self.channel_attention = ChannelAttention(out_c)\n","        self.spatial_attention = SpatialAttention()\n","\n","    def forward(self, inputs, skip):\n","        x = self.up(inputs)\n","        skip = self.dense(skip)  # Apply dense block to skip connection\n","        skip = self.attention(x, skip)  # Apply attention gate\n","        x = torch.cat([x, skip], axis=1)\n","        x = self.conv(x)\n","        x = self.channel_attention(x) * x\n","        x = self.spatial_attention(x) * x\n","        return x\n","\n","class Model(nn.Module):\n","    def __init__(self, encoder_cfg, bottleneck_cfg, decoder_cfg, output_channels, dropout_rate=0.4):\n","        super().__init__()\n","\n","        \"\"\" Encoder \"\"\"\n","        self.encoder_blocks = nn.ModuleList([\n","            encoder_block(encoder_cfg[i], encoder_cfg[i + 1], dropout_rate) for i in range(len(encoder_cfg) - 1)\n","        ])\n","\n","        \"\"\" Bottleneck with Residual Connection \"\"\"\n","        self.b_conv1 = conv_block(bottleneck_cfg[0], bottleneck_cfg[1], dropout_rate)\n","        self.b_conv2 = conv_block(bottleneck_cfg[1], bottleneck_cfg[1], dropout_rate)\n","        self.b_residual = nn.Conv2d(bottleneck_cfg[0], bottleneck_cfg[1], kernel_size=1)\n","        self.se = SEBlock(bottleneck_cfg[1])\n","        self.channel_attention = ChannelAttention(bottleneck_cfg[1])\n","        self.spatial_attention = SpatialAttention()\n","        self.ppm = PyramidPoolingModule(bottleneck_cfg[1], pool_sizes=[1, 2, 3, 6])\n","\n","        \"\"\" Decoder \"\"\"\n","        self.decoder_blocks = nn.ModuleList([\n","            decoder_block(decoder_cfg[i], decoder_cfg[i + 1], dropout_rate) for i in range(len(decoder_cfg) - 1)\n","        ])\n","\n","        \"\"\" Final Convolution \"\"\"\n","        self.final_conv = nn.Conv2d(decoder_cfg[-1], output_channels, kernel_size=1)\n","        self.dropout = nn.Dropout2d(dropout_rate)\n","\n","    def forward(self, inputs):\n","        \"\"\" Encoder \"\"\"\n","        skip_connections = []\n","        x = inputs\n","        for encoder in self.encoder_blocks:\n","            s, x = encoder(x)\n","            skip_connections.append(s)\n","\n","        \"\"\" Bottleneck with Residual Connection \"\"\"\n","        residual = self.b_residual(x)\n","        x = self.b_conv1(x)\n","        x = self.dropout(x)\n","        x = self.b_conv2(x) + residual\n","        x = self.se(x)\n","        x = self.channel_attention(x) * x\n","        x = self.spatial_attention(x) * x\n","        x = self.ppm(x)\n","\n","        \"\"\" Decoder \"\"\"\n","        skip_connections = skip_connections[::-1]\n","        for i in range(len(self.decoder_blocks)):\n","            x = self.decoder_blocks[i](x, skip_connections[i])\n","\n","        \"\"\" Final Convolution \"\"\"\n","        x = self.dropout(x)\n","        x = self.final_conv(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","This file needs to contain the main training loop. The training code should be encapsulated in a main() function to\n","avoid any global variables.\n","\"\"\"\n","        \n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from torchvision import datasets\n","import torchvision.transforms.v2 as transforms\n","\n","from torchvision.datasets import Cityscapes\n","from argparse import ArgumentParser\n","\n","import wandb\n","import torch.optim.lr_scheduler as lr_scheduler\n","import gc\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import random\n","\n","\n","try:\n","    import utils\n","except Exception:\n","    import sys\n","    sys.path.insert(1, '/kaggle/input/5lsm0-neural-networks-for-cv-dataset')\n","    import utils\n","\n","\n","def get_arg_parser():\n","    parser = ArgumentParser()\n","    parser.add_argument(\"--data_path\", type=str, default=\".\", help=\"Path to the data\")\n","    \"\"\"add more arguments here and change the default values to your needs in the run_container.sh file\"\"\"\n","    return parser\n","\n","\n","def generate_random_colormap(num_classes):\n","    colormap = {}\n","    for i in range(num_classes):\n","        colormap[i] = tuple(random.randint(0, 255) for _ in range(3))\n","    return colormap\n","\n","\n","# Then, use the colormap as before\n","def colorize_mask(mask,colormap):\n","    \"\"\"Convert a label mask to an RGB image.\"\"\"\n","    mask_colorized = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","    for label, color in colormap.items():\n","        mask_colorized[mask == label] = color\n","    return mask_colorized\n","\n","def visualize_images_and_masks(loader):\n","    dataiter = iter(loader)\n","    images, masks = dataiter.__next__()\n","\n","    fig = plt.figure(figsize=(16, 10))\n","    for idx in np.arange(4):\n","        ax1 = fig.add_subplot(2, 4, 2*idx+1, xticks=[], yticks=[])\n","        ax2 = fig.add_subplot(2, 4, 2*idx+2, xticks=[], yticks=[])\n","\n","        plt.sca(ax1)\n","        plt.imshow(np.transpose(images[idx].cpu(), (1, 2, 0)))\n","        if idx == 0:\n","            ax1.set_title('Images')\n","\n","        plt.sca(ax2)\n","        plt.imshow(masks[idx].squeeze().cpu(), cmap=\"gray\")\n","        if idx == 0:\n","            ax2.set_title('Masks')\n","    plt.show()\n","\n","\n","\n","import torch.nn.functional as F\n","\n","class FocalLoss(nn.modules.loss._WeightedLoss):\n","    def __init__(self, weight=None, gamma=2,reduction='mean'):\n","        super(FocalLoss, self).__init__(weight,reduction=reduction)\n","        self.gamma = gamma\n","        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n","        self.cross_entropy = nn.CrossEntropyLoss(ignore_index=255)\n","\n","    def forward(self, input, target):\n","\n","        ce_loss = self.cross_entropy(input,target)#F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight, ignore_index=255) \n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n","        return focal_loss\n","    \n","    \n","    \n","def compute_iou(preds, labels, num_classes):\n","    ious = []\n","    for cls in range(num_classes):\n","        pred_cls = preds == cls\n","        label_cls = labels == cls\n","        intersection = (pred_cls & label_cls).sum().item()\n","        union = (pred_cls | label_cls).sum().item()\n","        if union == 0:\n","            ious.append(float('nan'))\n","        else:\n","            ious.append(intersection / union)\n","    return ious\n","    \n","    \n","    \n","class EarlyStopper:\n","    def __init__(self, patience=1, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.min_validation_loss = float('inf')\n","\n","    def early_stop(self, validation_loss):\n","        if validation_loss < self.min_validation_loss:\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","        elif validation_loss > (self.min_validation_loss + self.min_delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False\n","    \n","    \n","def main(args):\n","    print(\"Starting the main method...\")\n","\n","    \n","    \"\"\"define your model, trainingsloop optimitzer etc. here\"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # You don't have a gpu, so use cpu\n","\n","    import os\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    from torchvision import datasets\n","    import torchvision.transforms.v2 as transforms\n","    from torchvision.datasets import Cityscapes\n","    from argparse import ArgumentParser\n","    import wandb\n","    import torch.optim.lr_scheduler as lr_scheduler\n","    import random\n","\n","    \n","    \n","    transform_data = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ColorJitter(contrast=0.7),\n","    transforms.RandomRotation([-120,120]),\n","    transforms.RandomHorizontalFlip(p=0.6),\n","    transforms.RandomVerticalFlip(p=0.6),\n","    transforms.GaussianBlur(3,sigma=(0.1, 5)),\n","\n","    transforms.ToTensor(),\n","    transforms.Lambda(lambda x: x / 255.0),  # Scale the tensor values to the range [0, 1]\n","    ])\n","\n","    # Apply transformations to the dataset\n","    try:\n","        transformed_dataset = Cityscapes(\"/kaggle/input/5lsm0-neural-networks-for-cv-dataset\", split='train', mode='fine', target_type='semantic',transforms=transform_data)#, transform=transform_data, target_transform=transform_target)\n","    except Exception:\n","        transformed_dataset = Cityscapes(args.data_path, split='train', mode='fine', target_type='semantic',transforms=transform_data)\n","\n","    \n","    try:\n","        transformed_test_dataset = Cityscapes(\"/kaggle/input/5lsm0-neural-networks-for-cv-dataset\", split='test', mode='fine', target_type='semantic',transforms= transform_data)#, transform=transform_data, target_transform=transform_target)\n","        val_loader =  DataLoader(transformed_test_dataset, batch_size=64, shuffle=False, num_workers=18)\n","    except Exception:\n","        val_loader=None\n","        \n","    \n","    split_point1 = int(0.7 * len(transformed_dataset))  # 70% for training\n","    split_point2 = int(0.9 * len(transformed_dataset))  # 20% for validation, 10% for testing\n","\n","    # Split the dataset\n","    transformed_train_dataset = torch.utils.data.Subset(transformed_dataset, range(split_point1))\n","    transformed_val_dataset = torch.utils.data.Subset(transformed_dataset, range(split_point1, split_point2))\n","    transformed_test = torch.utils.data.Subset(transformed_dataset, range(split_point2, len(transformed_dataset)))\n","\n","\n","    bs = 32\n","    print(\"Batch size \", bs)\n","    \n","    train_loader = DataLoader(transformed_train_dataset, batch_size=16, shuffle=True, num_workers=18)\n","    val_loader = DataLoader(transformed_val_dataset, batch_size=16, shuffle=True, num_workers=18)\n","    test_loader = DataLoader(transformed_test, batch_size=4, shuffle=False, num_workers=18)\n","\n","    \n","    \n","    encoder_cfg = [3, 64, 128, 256, 512]  # Example encoder configuration\n","    bottleneck_cfg = [512, 1024]  # Example bottleneck configuration\n","    decoder_cfg = [1024, 512, 256, 128, 64]  # Example decoder configuration\n","    \n","\n","    output_channels = 19  # Number of output channels for segmentation\n","\n"," \n","    try:\n","        model = Model(encoder_cfg, bottleneck_cfg, decoder_cfg, output_channels).cuda()\n","\n","        print(\"Using CUDA\")\n","        \n","    except Exception as exc:\n","        print(exc)\n","\n","        model = Model(encoder_cfg, bottleneck_cfg, decoder_cfg, output_channels)\n","\n","\n","        print(\"Using CPU\")\n","\n","    display(summary(model,(3,256, 256)))\n","\n","    num_epochs = 200\n","    print(num_epochs)\n","    learning_rate = 0.001\n","    print(\"USING \", learning_rate)\n","    \n","    best_train_loss= 100\n","    best_test_loss= 100\n","\n","    # Set the loss function and optimizer\n","    from torch import tensor\n","    from torchmetrics.classification import Dice\n","    from torchmetrics.functional.classification import dice\n","    criterion = nn.CrossEntropyLoss(ignore_index=255) \n","    wd = 0.01 \n","    print(\"WD: \", wd)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=wd,amsgrad=True)\n","    \n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","    \n","    print(scheduler.get_last_lr(), \" LEARNING RATE\")\n","    count = 0\n","    loss_list = []\n","    iteration_list = []\n","    # Train the model\n","    \n","    \n","    def generate_random_colormap(num_colors):\n","        np.random.seed(0)\n","        colormap = np.random.randint(0, 255, size=(num_colors, 3), dtype=np.uint8)\n","        return colormap\n","\n","    def colorize_mask(mask, colormap):\n","        colorized = colormap[mask]\n","        return colorized\n","    \n","    \n","    colormap = generate_random_colormap(256)\n","    \n","    early_stopper = EarlyStopper(patience=20, min_delta=10)\n","\n","    for epoch in range(num_epochs): \n","        running_loss = 0.0\n","        test_loss=0.0\n","        model.train()\n","        for i, data in enumerate(train_loader):\n","            inputs, labels = data[0].to(device), (data[1]*255).to(device).long()\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            labels = labels.squeeze(1)  # remove the extra dimension\n","            labels = utils.map_id_to_train_id(labels).to(device)\n","            print(labels.unique())\n","            loss = criterion(outputs, labels)\n","            v=epoch + 1\n","            \n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            print(f'Epoch {epoch + 1}, Iteration [{i}/{len(train_loader)}], Loss: {running_loss/(i+1)}, Model is in mode training: {model.training}',outputs.shape,inputs.shape)\n","\n","            \n","        clear_output()\n","        \n","        if((running_loss / len(train_loader))<best_train_loss):\n","            best_train_loss=running_loss / len(train_loader)\n","            print(f\"Updated train loss, now it is {best_train_loss}\")\n","        print(f'Finished epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n","\n","        with torch.no_grad():\n","            model.eval()\n","            \n","            for i, data in enumerate(test_loader):\n","\n","                inputs, labels = data[0].to(device), (data[1]*255).to(device).long()\n","                labels=labels.squeeze(1)\n","                labels = utils.map_id_to_train_id(labels).to(device)\n","                outputs = model(inputs)\n","\n","\n","                loss = criterion(outputs, labels)\n","                v=epoch + 1\n","\n","\n","                test_loss += loss.item()\n","                preds = torch.argmax(outputs, dim=1)\n","                ious = compute_iou(preds.cpu().numpy(), labels.cpu().numpy(), output_channels)\n","\n","                print(f'TEST: Epoch {epoch + 1}, Iteration [{i}/{len(test_loader)}], Loss: {running_loss/(i+1)}, Model is in mode training: {model.training}') #,outputs.shape,inputs.shape\n","\n","            if((test_loss / len(test_loader))<best_test_loss):\n","                best_test_loss=test_loss / len(test_loader)\n","                print(f\"Updated test loss, now it is {best_test_loss}\")\n","                \n","        clear_output()\n","        \n","        scheduler.step()\n","        print(scheduler.get_last_lr(), \" NEW LEARNING RATE\")\n","        print(f'Finished TEST epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(test_loader):.4f}')\n","\n","        \n","        \n","\n","        # Visualize the first image in the batch\n","        if i % 1 == 0:  # Visualize every 5 iterations\n","            import matplotlib.pyplot as plt\n","            import numpy as np\n","\n","            def generate_random_colormap(num_classes):\n","                colormap = {}\n","                for i in range(num_classes):\n","                    colormap[i] = tuple(random.randint(0, 255) for _ in range(3))\n","                return colormap\n","\n","            def colorize_mask(mask):\n","                \"\"\"Convert a label mask to an RGB image.\"\"\"\n","                mask_colorized = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","                for label, color in colormap.items():\n","                    mask_colorized[mask == label] = color\n","                return mask_colorized\n","\n","            model.eval()\n","            predicted = torch.argmax(outputs,dim=1)\n","            print(predicted.unique())\n","            img = inputs.cpu().numpy()[0]\n","            img = np.transpose(img, (1, 2, 0))\n","            img = img * 255  # Rescale the image back to 0-255\n","            img = np.clip(img, 0, 255)\n","\n","            label = labels.cpu().numpy()[0]\n","            pred = predicted.cpu().numpy()[0]\n","\n","            print(\"Ground truth label range:\", np.min(label), np.max(label))\n","            print(\"Predicted label range:\", np.min(pred), np.max(pred))\n","\n","            # Generate a random colormap for your number of classes\n","            num_classes = len(np.unique(label))  # Change this to the number of classes in your dataset\n","            colormap = generate_random_colormap(num_classes)\n","\n","            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","            axs[0].imshow(img)\n","            axs[0].set_title('Input Image')\n","            axs[1].imshow(colorize_mask(label))\n","            axs[1].set_title('Ground Truth')\n","            axs[2].imshow(colorize_mask(pred))\n","            axs[2].set_title('Prediction')\n","\n","            plt.show()\n","            model.train()\n","\n","\n","        if early_stopper.early_stop(test_loss):             \n","            break\n","\n","        \n","    if (val_loader):\n","        with torch.no_grad():\n","                model.eval()\n","\n","                for i, data in enumerate(val_loader):\n","                    inputs, labels = data[0].to(device), (data[1]*255).to(device).long()\n","                    labels=labels.squeeze(1)\n","\n","                    outputs = model(inputs)\n","\n","                    loss = criterion(outputs, labels)\n","\n","                    running_loss += loss.item()\n","\n","                    print(f'VAL: Iteration [{i}/{len(val_loader)}], Loss: {running_loss/(i+1)}',outputs.shape,inputs.shape)\n","    # save model\n","    print(f\"Best losses --> Train:{best_train_loss} and Test:{best_test_loss}\")\n","    torch.save(model.state_dict(), \"model_scripted.pth\")\n","    \n","\n","\n","\n","    # visualize some results\n","\n","    pass\n","\n","\n","if __name__ == \"__main__\":\n","    gc.collect()\n","    main(\"\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4515724,"sourceId":7841762,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
